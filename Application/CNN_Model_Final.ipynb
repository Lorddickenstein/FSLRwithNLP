{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Model_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorddickenstein/FSLRwithNLP/blob/main/Application/CNN_Model_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8h3-OVgKC7x"
      },
      "source": [
        "# Convolutional Neural Network with Own Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW3XEfE0B6ei"
      },
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3OXdGaDI2JC",
        "outputId": "2052ae5a-4713-4767-e179-175d2836769f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TagkyO_C5Zs"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Activation, Dense, Flatten, MaxPool2D, Conv2D, Dropout, BatchNormalization\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import warnings\n",
        "import random\n",
        "import shutil\n",
        "import itertools\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mOl456BLq7Y"
      },
      "source": [
        "## Do you have GPU?\n",
        "If using a GPU, run this to make the computer know that you have a gpu so tensorflow could identify it correctly and enable memory growth on the gpu.\n",
        "\n",
        "Otherwise, leave this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20u7VbPOKd4U"
      },
      "source": [
        "physical_devices = tf.config.expertimental.list_physical_devices('GPU')\n",
        "print('Nump GPUs Available: ', len(physical_devices))\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jxl45mTMeUu"
      },
      "source": [
        "# Data Preparation\n",
        "The images are found inside the OurDataset Folder under Raw_Dataset. Images will be read by the program and the output will be placed into the inside the FSLR_Application_Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOeThNVWCZ6n"
      },
      "source": [
        "# paths and directories\n",
        "root = '/content/drive/MyDrive/Colab Notebooks/Datasets/FSLR_Application_Dataset'\n",
        "dataset_root = '/content/drive/MyDrive/Colab Notebooks/Datasets/FSLR_Application_Dataset/Preprocessed_Raw_Dataset'\n",
        "\n",
        "train_path = os.path.join(root, \"Train\")\n",
        "valid_path = os.path.join(root, \"Valid\")\n",
        "test_path = os.path.join(root, \"Test\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1BjZQuFMcjv"
      },
      "source": [
        "# Organize data into train, valid, test dirs\n",
        "categories = ['Dynamic Single', 'Dynamic Double',\n",
        "                       'Static Single', 'Static Double',\n",
        "                       'Letters', 'Numbers']\n",
        "\n",
        "letters = ['A', 'B', 'C', 'D', 'E',\n",
        "           'F', 'G', 'H', 'I', 'J',\n",
        "           'K', 'L', 'M', 'N', 'O',\n",
        "           'P', 'Q', 'R', 'S', 'T',\n",
        "           'U', 'V', 'W', 'X', 'Y',\n",
        "           'Z']\n",
        "\n",
        "numbers = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "static_single = ['Fine', 'Gabi', 'Good', 'Hapon', 'He-She',\n",
        "                 'His-Her', 'I Love You', 'I-Me', 'Mine',\n",
        "                 'Tanghali', 'Umaga', 'You', 'Your']\n",
        "\n",
        "static_double = ['Congratulations', 'Great', 'Help', 'Meet',\n",
        "                 'Name', 'Night', 'Occupation', 'Pray', 'Rest',\n",
        "                 'Stand', 'Study', 'To']\n",
        "\n",
        "dynamic_single = ['Eroplano', 'Eroplano-2', 'Invite', 'Late', 'Late-2',\n",
        "                  'No', 'No-2', 'Our', 'Our-2', 'Sorry', 'That', 'Them',\n",
        "                  'This', 'We', 'Welcome', 'Welcome-2', 'When', 'Who',\n",
        "                  'Who-2', 'Why', 'Why-2', 'Yes', 'Yesterday']\n",
        "\n",
        "dynamic_double = ['Ago', 'Allow', 'Ball', 'Banana', 'Banana-2', 'Bread', 'Break',\n",
        "                  'Break-2', 'Bring', 'Bring-2', 'Buy', 'Buy-2', 'Bye', 'Coconut',\n",
        "                  'Coffee', 'Come', 'Come-2', 'Cook', 'From', 'From-2', 'Get', 'Get-2',\n",
        "                  'Go', 'Go-2', 'Happen', 'Happen-2', 'How', 'How-2', 'Introduce', 'Introduce-2',\n",
        "                  'Let', 'Let-2', 'Live', 'Mango', 'Maybe', 'Nice', 'Now', 'Office', 'Office-2',\n",
        "                  'School', 'Sit', 'Sit-2', 'Store', 'Strawberry', 'Thank You', 'Thank You-2', 'Today', 'Today-2',\n",
        "                  'What', 'Where', 'Which', 'Work', 'Year']\n",
        "\n",
        "dataset_classes = letters + numbers + static_single + static_double + dynamic_single + dynamic_single"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foaLqRPSORak",
        "outputId": "1323e15a-8115-4168-8364-e6d618aa10ef"
      },
      "source": [
        "print(len(letters))\n",
        "print(len(numbers))\n",
        "print(len(static_single))\n",
        "print(len(static_double))\n",
        "print(len(dynamic_single))\n",
        "print(len(dynamic_double))\n",
        "print(len(dataset_classes))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "9\n",
            "13\n",
            "12\n",
            "23\n",
            "53\n",
            "106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMjyDTFWOT7i"
      },
      "source": [
        "assert len(letters) == 26\n",
        "assert len(numbers) == 9\n",
        "assert len(static_single) == 13\n",
        "assert len(static_double) == 12\n",
        "assert len(dynamic_single) == 23\n",
        "assert len(dynamic_double) == 53"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4LlhO4fLIWW"
      },
      "source": [
        "## Functions\n",
        "Contains all the functions that are needed for this program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6bTYwF5QMdm"
      },
      "source": [
        "# Normalize images\n",
        "def preprocess_func(src_img):\n",
        "  norm = src_img.astype('float32')\n",
        "  norm /= 255\n",
        "  return norm"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axexCmt1Q-hh"
      },
      "source": [
        "# Plot 10 sample images\n",
        "def plotImages(images_arr):\n",
        "  fig, axes = plt.subplots(1, 10, figsize=(20, 20))\n",
        "  axes = axes.flatten()\n",
        "  for img, ax in zip(images_arr, axes):\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqRRbiWPSs64"
      },
      "source": [
        "# Create the Sequential Model\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  # Layers\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(120, 120, 3), padding='same'))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.20))\n",
        "  model.add(Dense(total_classes, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXZBnEbmbjT8"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GriuJ4qcTLer"
      },
      "source": [
        "# Populate **Train**, **Valid**, and **Test** Folders\n",
        "Populate the **Train, Valid, and Test** folder with directories of classes. Uses ***random sampling*** to randomize selection of images and uses ***glob*** to select file paths matching specific pattern in their names. ***shutil*** module allows the transfer of these files to the *Train, Valid, and Test*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_imDzItM2BT"
      },
      "source": [
        "## Populate Train Folder\n",
        "Copy all images from preprocessed datasets folder to train folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQds37ajDM2r"
      },
      "source": [
        "# Letters\n",
        "dataset_path = os.path.join(dataset_root, 'Letters')\n",
        "for sign in letters:\n",
        "  path_class_dest = os.path.join(train_path, sign)\n",
        "\n",
        "  if os.path.isdir(path_class_dest) is False:\n",
        "    os.makedirs(path_class_dest)\n",
        "    \n",
        "  path_class = os.path.join(dataset_path, sign)\n",
        "  for item in os.listdir(path_class):\n",
        "    shutil.copy(os.path.join(path_class, item), path_class_dest)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h0VXhZ6MTMF"
      },
      "source": [
        "# Numbers\n",
        "dataset_path = os.path.join(dataset_root, 'Numbers')\n",
        "for sign in numbers:\n",
        "  path_class_dest = os.path.join(train_path, sign)\n",
        "\n",
        "  if os.path.isdir(path_class_dest) is False:\n",
        "    os.makedirs(path_class_dest)\n",
        "\n",
        "  path_class = os.path.join(dataset_path, sign)\n",
        "  for item in os.listdir(path_class):\n",
        "    shutil.copy(os.path.join(path_class, item), path_class_dest)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIxDsB70NDBf"
      },
      "source": [
        "# Static Single\n",
        "dataset_path = os.path.join(dataset_root, 'Static Single')\n",
        "for sign in static_single:\n",
        "  path_class_dest = os.path.join(train_path, sign)\n",
        "\n",
        "  if os.path.isdir(path_class_dest) is False:\n",
        "    os.makedirs(path_class_dest)\n",
        "\n",
        "  path_class = os.path.join(dataset_path, sign)\n",
        "  for item in os.listdir(path_class):\n",
        "    shutil.copy(os.path.join(path_class, item), path_class_dest)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHwflZBRNOWP"
      },
      "source": [
        "# Static Double\n",
        "dataset_path = os.path.join(dataset_root, 'Static Double')\n",
        "for sign in static_double:\n",
        "  path_class_dest = os.path.join(train_path, sign)\n",
        "\n",
        "  if os.path.isdir(path_class_dest) is False:\n",
        "    os.makedirs(path_class_dest)\n",
        "    \n",
        "  path_class = os.path.join(dataset_path, sign)\n",
        "  for item in os.listdir(path_class):\n",
        "    shutil.copy(os.path.join(path_class, item), path_class_dest)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM2SEPAwNUeT"
      },
      "source": [
        "# Dynamic Single\n",
        "dataset_path = os.path.join(dataset_root, 'Dynamic Single')\n",
        "for sign in dynamic_single:\n",
        "  path_class_dest = os.path.join(train_path, sign)\n",
        "\n",
        "  if os.path.isdir(path_class_dest) is False:\n",
        "    os.makedirs(path_class_dest)\n",
        "    \n",
        "  path_class = os.path.join(dataset_path, sign)\n",
        "  for item in os.listdir(path_class):\n",
        "    shutil.copy(os.path.join(path_class, item), path_class_dest)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRZsg2cDNafR"
      },
      "source": [
        "# Dynamic Double\n",
        "dataset_path = os.path.join(dataset_root, 'Dynamic Double')\n",
        "for sign in dynamic_double:\n",
        "  path_class_dest = os.path.join(train_path, sign)\n",
        "\n",
        "  if os.path.isdir(path_class_dest) is False:\n",
        "    os.makedirs(path_class_dest)\n",
        "    \n",
        "  path_class = os.path.join(dataset_path, sign)\n",
        "  for item in os.listdir(path_class):\n",
        "    shutil.copy(os.path.join(path_class, item), path_class_dest)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlNlx1-jOD99"
      },
      "source": [
        "## Populate Valid and Test Folder\n",
        "Take samples from Train folder and put them into Valid and Test Folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXRxb9PzPKvT"
      },
      "source": [
        "# Valid samples = 50\n",
        "valid_size = 50\n",
        "  \n",
        "#Test samples = 20\n",
        "test_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuCHvfnYFgtJ"
      },
      "source": [
        "# Move samples from Train folder\n",
        "for sign in dataset_classes:\n",
        "  path_class = os.path.join(train_path, sign)\n",
        "  os.chdir(path_class)\n",
        "\n",
        "  if len(os.listdir(path_class)) != 0:\n",
        "    # Move 50 images from Train to Valid folder\n",
        "    for item in random.sample(glob.glob(sign + '_*'), valid_size):\n",
        "      shutil.move(item, os.path.join(valid_path, sign))\n",
        "\n",
        "    # Move 20 images from Train to Test folder\n",
        "    for item in random.sample(glob.glob(sign + '_*'), test_size):\n",
        "      shutil.move(item, os.path.join(test_path, sign))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9RwawerVCoB"
      },
      "source": [
        "# Verify number of images in all folders\n",
        "for sign in dataset_classes:\n",
        "  train_path_experiment = os.path.join(train_path, sign)\n",
        "  valid_path_experiment = os.path.join(valid_path, sign)\n",
        "  test_path_experiment = os.path.join(test_path, sign)\n",
        "  print(sign, len(os.listdir(train_path_experiment)), len(os.listdir(valid_path_experiment)), len(os.listdir(test_path_experiment)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwVQWUbWx9m"
      },
      "source": [
        "# Preprocess Image\n",
        "Transform images from the dataset into a format that the model expect. Applies data augmentation to increase the number of datasets used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crCSekUeYGMm"
      },
      "source": [
        "image_size = (120, 120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6QQD4P7XY2Z"
      },
      "source": [
        "# Augments dataset 10x\n",
        "train_batches = ImageDataGenerator(preprocessing_function=preprocess_func, horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, fill_mode='nearest') \\\n",
        "    .flow_from_directory(directory=train_path, target_size=image_size, classes=dataset_classes, batch_size=10)\n",
        "valid_batches = ImageDataGenerator(preprocessing_function=preprocess_func,horizontal_flip=True, width_shift_range=0.15, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, fill_mode='nearest') \\\n",
        "    .flow_from_directory(directory=valid_path, target_size=image_size, classes=dataset_classes, batch_size=10)\n",
        "test_batches = ImageDataGenerator(preprocessing_function=preprocess_func,horizontal_flip=True, width_shift_range=0.15, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, fill_mode='nearest') \\\n",
        "    .flow_from_directory(directory=test_path, target_size=image_size, classes=dataset_classes, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpAhrgnfYx9F"
      },
      "source": [
        "assert valid_batches.n == 50\n",
        "assert test_batches.n == 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlPIF7d2ZuFm"
      },
      "source": [
        "imgs, labels = next(train_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R18knn9YZoxo"
      },
      "source": [
        "plotImages(imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPRXtDqVqWW"
      },
      "source": [
        "# Save checkpoints during training\n",
        "##Employing the following:\n",
        "\n",
        "1. Checkpoints\n",
        "\n",
        "2. CSV Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MJcUqbKVocz"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/Datasets/CNN Model/weights_improvements-epoch:{epoch:02d}-val_accuracy:{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                              verbose=1,\n",
        "                              monitor='val_accuracy',\n",
        "                              mode='max',\n",
        "                              save_best_only=True,\n",
        "                              period=5)\n",
        "\n",
        "log_folder = '/content/drive/MyDrive/Colab Notebooks/Datasets/CNN Model'\n",
        "log_path = os.path.join(log_folder, 'FSLR_logs.csv')\n",
        "log_csv = CSVLogger(log_path, separator=',', append=False)\n",
        "\n",
        "callback_list = [cp_callback, log_csv]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkCeD-cFZnow"
      },
      "source": [
        "# Building the model\n",
        "\n",
        "Model #1 to create a new model from scratch.\n",
        "\n",
        "Model #2 to resume the training for when the training is disrupted, stopped or first set of epoch finished. Training can continue in another time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN6hXpnWST83"
      },
      "source": [
        "# Count the total classes that the model must know\n",
        "total_classes = len(os.listdir(train_path))\n",
        "assert len(dataset_classes) = total_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRB3tdInb0nt"
      },
      "source": [
        "batch_size = 20\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd7u-bkBWr_-"
      },
      "source": [
        "## Model #1 New Model\n",
        "Choose only one model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy2VSh6_TNNF"
      },
      "source": [
        "# Create Model function\n",
        "model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlwhyurfYr38"
      },
      "source": [
        "# Summary of layers\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-9Pwrm3XRmP"
      },
      "source": [
        "# Compile the layers into one model and create a connection\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmveCMpvZALZ"
      },
      "source": [
        "# Train the model with the new callback\n",
        "history = model.fit(x=train_batches,\n",
        "                    validation_data=valid_batches,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callback_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEyNJNGjXDNK"
      },
      "source": [
        "## Model #2 Resume Training\n",
        "Choose only one model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP1FQurxXoIB"
      },
      "source": [
        "# Model configuration\n",
        "new_model_name = '' # must be in this format: 'FSLR_CNN_Model(02-epochs)-accuracy:0.00-val_accuracy:0.00.h5'\n",
        "new_path = '/content/drive/MyDrive/Colab Notebooks/Datasets/CNN Model'\n",
        "new_model_path = os.path.join(new_path, new_model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3WoUl2VW7z-"
      },
      "source": [
        "# Load Model\n",
        "new_model = load_model(new_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co5KkWuMXNLh"
      },
      "source": [
        "# Test model before resuming training\n",
        "print(new_model.evaluate(test_batches, verbose=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjeuruZPVAYr"
      },
      "source": [
        "# Create the connection and train the model\n",
        "new_model.fit(x=train_batches,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=valid_batches,\n",
        "          callbacks=callback_list)\n",
        "\n",
        "# Evaluate the model with test_sets\n",
        "print(new_model.evaluate(test_batches))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke9mbWJjavfN"
      },
      "source": [
        "new_model.save(new_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pf5SEbpSvNa"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmUUYWdYP6eb"
      },
      "source": [
        "# Model configurations\n",
        "model_name = 'FSLR_CNN_Model(00-epochs)-accuracy:0.00-val_accuracy:0.00.h5'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Datasets/CNN Model'\n",
        "model_path = os.path.join(path, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFfDwpRfZ-ux"
      },
      "source": [
        "# save the model\n",
        "model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSA5oeG_bGo3"
      },
      "source": [
        "# Confusion Matrix\n",
        "Plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBNn_1iWbGPt"
      },
      "source": [
        "# Create a prediction\n",
        "predictions = model.predict(x=test_batches, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xEv2Luobc6f"
      },
      "source": [
        "# Setup the confusion matrix\n",
        "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFjaAn6Sben9"
      },
      "source": [
        "# Check class indices\n",
        "test_batches.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt7oSiagbgBz"
      },
      "source": [
        "# Plot the confusion matrix\n",
        "cm_plot_labels = total_classes\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}