{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CFG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsSdANI/3wGj3l0LWiin/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorddickenstein/FSLRwithNLP/blob/main/Application/NLP/CFG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMQiiBYiWCkD"
      },
      "source": [
        "import nltk\n",
        "from nltk import Tree\n",
        "from subprocess import *\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> QP | VP SP | QP SP | O VP | WP | O JJ \n",
        "  SP -> PRP NN | PRP IN | NN | PRP |\n",
        "  QP -> WP NN | WRB A2 | WP | WRB\n",
        "  O -> SP A1 | SP A2 A3 | SP\n",
        "  VP -> VB RB | VB WRB | VB TO | QP\n",
        "  A1 -> \"is\"\n",
        "  A2 -> \"are\"\n",
        "  A3 -> \"am\"\n",
        "  IN -> \"from\"\n",
        "  JJ -> \"good\"\n",
        "  WP -> \"what\" | \"when\"\n",
        "  WDT -> \"which\"\n",
        "  WRB -> \"where\" | \"how\"\n",
        "  NN -> \"name\" | \"okay\" | \"office\" | \"work\"\n",
        "  PRP -> \"you\" | \"i\"\n",
        "  RB -> \"here\" \n",
        "  TO -> \"to\"\n",
        "  VB -> \"live\" | \"go\"\n",
        "  \"\"\")\n",
        "\n",
        "grammar9 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> QP | SP VP | SP JJ | SP PNN | VP PP | SP | SP PP\n",
        "  QP -> SP WQ | SP PP WQ | SP VP WQ\n",
        "  SP -> PRP NN | PRP | NN | NN SP\n",
        "  VP -> VB RB | VB \n",
        "  PP -> IN SP | IN \n",
        "  WQ -> WP | WRB\n",
        "  NN -> 'name' | 'egg' | 'office' | 'work' | 'person' | 'student' \n",
        "  PRP -> 'you' | 'i-me' | 'he-she'\n",
        "  WP -> 'what' | 'who' | 'when'\n",
        "  WRB -> 'how' | 'where'\n",
        "  JJ -> 'good' | 'okay'\n",
        "  IN -> 'from' | 'to'\n",
        "  RB -> 'here'\n",
        "  VB -> 'live' | 'cook' | 'go' | 'study'\n",
        "  NNP -> letter NNP | letter\n",
        "  letter -> 'J' | 'E' | 'R' | 'S'\n",
        "  PNN -> 'jers'\n",
        "\"\"\")\n",
        "# (S -> SP VP) (SP -> NN SP) (SP -> PRP) (VP -> VB) \n",
        "grammar10 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> QP | SP VP | SP JJ | SP NNP | VP PP |\n",
        "  QP -> WQ SP | WQ SP PP | WQ SP VP \n",
        "  SP -> PRPS NN | PRP FWA | NN | NN SP | PRP | PRPS NN FWA \n",
        "  VP -> VB RB | VB NN | VB\n",
        "  PP -> IN DT SP | IN SP | IN\n",
        "  WQ -> WP FWA | WRB | WRB FWC | WRB FWA\n",
        "  FWA -> 'is' | 'are' | 'am'\n",
        "  FWC -> 'do' \n",
        "  NN -> 'name' | 'egg' | 'office' | 'work'\n",
        "  PRPS -> 'your' | 'my'\n",
        "  PRP -> 'you' | 'i-me' | 'he-she'\n",
        "  WP -> 'what' | 'who' | 'when'\n",
        "  WRB -> 'how' | 'where'\n",
        "  JJ -> 'good' | 'okay'\n",
        "  IN -> 'from' | 'to'\n",
        "  RB -> 'here'\n",
        "  VB -> 'live' | 'cook' | 'go'\n",
        "  DT -> 'the' | 'an' | 'a'\n",
        "  NNP -> 'jers' \n",
        "\"\"\")\n",
        "\n",
        "# (S (QP (WP What)) (SP (PRP you) (NN name)))\n",
        "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> QP | VP SP | QP A1 SP | O SP | O VBP\n",
        "  SP -> NN | PRP NN \n",
        "  QP ->  WP\n",
        "  WP -> \"what\"\n",
        "  A1 -> \"is\"\n",
        "  NN -> \"name\"\n",
        "  PRP -> \"your\"\n",
        "  \"\"\")\n",
        "\n",
        "grammar4 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> A\n",
        "  A -> QP SP  \n",
        "  SP -> NN\n",
        "  QP -> WP O\n",
        "  O -> PRP\n",
        "  WP -> \"what\"\n",
        "  PRP -> \"your\"\n",
        "  NN -> \"name\"\n",
        "  A1 -> \"is\"\n",
        "  \"\"\")\n",
        "\n",
        "grammar7= nltk.CFG.fromstring(\"\"\"\n",
        "  S -> A\n",
        "  A -> QP SP  \n",
        "  SP -> NN\n",
        "  QP -> WP O\n",
        "  O -> PRP\n",
        "  WP -> \"what\"\n",
        "  PRP -> \"your\"\n",
        "  NN -> \"name\"\n",
        "  A1 -> \"is\"\n",
        "  \"\"\")\n",
        " \t\n",
        "grammar3 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  V -> \"saw\" | \"ate\" | \"walked\"\n",
        "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
        "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "  \"\"\")\n",
        "\n",
        "grammar6 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  PP -> P NP\n",
        "  NP -> Det N | Det N PP\n",
        "  VP -> V NP | VP PP\n",
        "  Det -> 'DT'\n",
        "  N -> 'NN'\n",
        "  V -> 'VBZ'\n",
        "  P -> 'PP'\n",
        "  \"\"\")\n",
        "# What's your name (you name what?) - (S (O (SP (PRP you) (NN name))) (VBP (QP (WP what))))\n",
        "# I'm good (I good) - (S (O (SP (PRP i))) (JJ good))\n",
        "# Where you from (you from where) -> (S (O (SP (PRP you) (IN from))) (VBP (QP (WRB where))))\n",
        "# I live here -> (S (O (SP (PRP i))) (VBP (VB live) (RB here)))\n",
        "# How are you (How you?) -> (S (QP (WRB how)) (SP (PRP you)))\n",
        "# Where do you live? (you, live, where?) -> (S (O (SP (PRP you))) (VBP (VB live) (WRB where)))\n",
        "# Are you okay? (you, okay) ->(S (O (SP (PRP you) (NN okay))))\n",
        "# My name is (I-Me, name) -> (S (O (SP (PRP i) (NN name))))\n",
        "# Go to the Office (Go, to, Office) -> (S (VP (VB go) (TO to)) (SP (NN office)))\n",
        "# Go to work (Go, to, work) -> (S (VBP (VB go) (TO to)) (SP (NN work)))"
      ],
      "execution_count": 531,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcHvdJvGlNPp"
      },
      "source": [
        "data_list = ['name', 'egg','office','work','person','student','you','i-me','he-she','what','who','when','how','where','good','okay','from','to','here','live',\n",
        "             'cook','go','study']\n",
        "\n",
        "\n",
        "def update_grammar(word='lagoon'):\n",
        "  grammar = f\"\"\"\n",
        "    S -> QP | SP VP | SP JJ | SP NNP | VP PP | SP\n",
        "    QP -> SP WQ | SP PP WQ | SP VP WQ\n",
        "    SP -> PRP NN | PRP | NN | NN SP \n",
        "    VP -> VB RB | VB \n",
        "    PP -> IN SP | IN\n",
        "    WQ -> WP | WRB\n",
        "    NN -> 'name' | 'egg' | 'office' | 'work' | 'person' | 'student' \n",
        "    PRP -> 'you' | 'i-me' | 'he-she'\n",
        "    WP -> 'what' | 'who' | 'when'\n",
        "    WRB -> 'how' | 'where'\n",
        "    JJ -> 'good' | 'okay'\n",
        "    IN -> 'from' | 'to'\n",
        "    RB -> 'here'\n",
        "    VB -> 'live' | 'cook' | 'go' | 'study' | 'come'\n",
        "    NNP -> {word} \n",
        "  \"\"\"\n",
        "\n",
        "  return grammar"
      ],
      "execution_count": 561,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ9PAlsHttOD",
        "outputId": "58948bed-2e13-45fb-e857-c0badd27f65e"
      },
      "source": [
        "grammar1.productions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[S -> QP,\n",
              " S -> VP SP,\n",
              " S -> QP SP,\n",
              " S -> O VBP,\n",
              " S -> WP,\n",
              " S -> O JJ,\n",
              " S -> VBP SP,\n",
              " SP -> PRP NN,\n",
              " SP -> PRP IN,\n",
              " SP -> NN,\n",
              " SP -> PRP,\n",
              " SP -> ,\n",
              " QP -> WP NN,\n",
              " QP -> WRB A2,\n",
              " QP -> WP,\n",
              " QP -> WRB,\n",
              " O -> SP A1,\n",
              " O -> SP A2 A3,\n",
              " O -> SP,\n",
              " VBP -> VB RB,\n",
              " VBP -> VB WRB,\n",
              " VBP -> VB TO,\n",
              " VBP -> QP,\n",
              " A1 -> 'is',\n",
              " A2 -> 'are',\n",
              " A3 -> 'am',\n",
              " IN -> 'from',\n",
              " JJ -> 'good',\n",
              " WP -> 'what',\n",
              " WP -> 'when',\n",
              " WDT -> 'which',\n",
              " WRB -> 'where',\n",
              " WRB -> 'how',\n",
              " NN -> 'name',\n",
              " NN -> 'okay',\n",
              " NN -> 'office',\n",
              " NN -> 'work',\n",
              " PRP -> 'you',\n",
              " PRP -> 'i',\n",
              " RB -> 'here',\n",
              " TO -> 'to',\n",
              " VB -> 'live',\n",
              " VB -> 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdzz078kQLoz",
        "outputId": "efb5ac62-ce78-449f-aa0b-ba5115d64a2c"
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "d = {}\n",
        " \n",
        "d[\"s\"]=[[\"a\", \"b\"]]\n",
        "d[\"a\"]=[[\"what\", \"e\"],[\"when\", \"e\"]]\n",
        "d[\"b\"]=[[\"c\", \"d\"]]\n",
        "d[\"c\"]=[[\"you\"]]\n",
        "d[\"d\"]=[[\"name\"],[\"bday\"]]\n",
        "d[\"e\"]=[[\"is\"]]\n",
        "\n",
        "expansion = []\n",
        "seed = \"s\"\n",
        "\n",
        "\n",
        "def expand(start, expansion):\n",
        "    if start in d:\n",
        "    #Grab one possible expansion\n",
        "        possible = d[start]\n",
        "        random_expansion = random.choice(possible)\n",
        "    # Call this method again with the current element\n",
        "        for i in range(len(random_expansion)):\n",
        "            expand(random_expansion[i], expansion);\n",
        " \n",
        "  # if the rule wasn't found, then it's a terminal:\n",
        "  # append the string to the expansion\n",
        "    else:\n",
        "        expansion.append(start)\n",
        "\n",
        "expand(seed, expansion)\n",
        "\n",
        "print(expansion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['when', 'is', 'you', 'bday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E53LOjHJvsle"
      },
      "source": [
        "' +|[A-Za-z]+|[()]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chdxWByqOJse"
      },
      "source": [
        "import re\n",
        "\n",
        "class ParseError(Exception):\n",
        "    pass\n",
        "\n",
        "# Tokenize a string.\n",
        "# Tokens yielded are of the form (type, string)\n",
        "# Possible values for 'type' are '(', ')' and 'WORD'\n",
        "def tokenize(s, reg_ex):\n",
        "    toks = re.compile(reg_ex)\n",
        "    for match in toks.finditer(s):\n",
        "        s = match.group(0)\n",
        "        if s[0] == ' ':\n",
        "            continue\n",
        "        if s[0] in '()':\n",
        "            yield (s, s)\n",
        "        else:\n",
        "            yield ('word', s)\n",
        "\n",
        "\n",
        "# Parse once we're inside an opening bracket.\n",
        "def parse_inner(toks):\n",
        "    ty, name = next(toks)\n",
        "    if ty != 'word': raise ParseError\n",
        "    children = []\n",
        "    while True:\n",
        "        ty, s = next(toks)\n",
        "        if ty == '(':\n",
        "            children.append(parse_inner(toks))\n",
        "        elif ty == ')':\n",
        "            return (name, children)\n",
        "\n",
        "# Parse this grammar:\n",
        "# ROOT ::= '(' INNER\n",
        "# INNER ::= WORD ROOT* ')'\n",
        "# WORD ::= [A-Za-z]+\n",
        "def parse_root(toks):\n",
        "    ty, _ = next(toks)\n",
        "    if ty != '(': raise ParseError\n",
        "    return parse_inner(toks)\n",
        "\n",
        "# def show_children(tree):\n",
        "#     name, children = tree\n",
        "#     if not children: return\n",
        "#     print (\"%s -> %s\" % (name, ' '.join(child[0] for child in children)))\n",
        "#     for child in children:\n",
        "#         show_children(child)\n",
        "\n",
        "# example = '(S (QP (SP (PRP you) (NN name)) (WQ (WP what))))'\n",
        "# # show_children(parse_root(tokenize(example)))"
      ],
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln-W6TyiiWpf"
      },
      "source": [
        "def show_children(tree):\n",
        "    name, children = tree\n",
        "    if not children: return\n",
        "    print (\"%s -> %s\" % (name, ' '.join(child[0] for child in children)))\n",
        "    for child in children:\n",
        "        show_children(child)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFPN46dTf_jz"
      },
      "source": [
        "def show_children(tree, pattern):\n",
        "    name, children = tree\n",
        "    if not children: return \"\"\n",
        "    pattern = \"(%s -> %s) \" % (name, ' '.join(child[0] for child in children))\n",
        "    for child in children:\n",
        "      pattern += show_children(child, pattern)\n",
        "    # return children, (\"(%s -> %s) \" % (name, ' '.join(child[0] for child in children)))\n",
        "    return pattern\n",
        "\n"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujV5vCFH29Fr"
      },
      "source": [
        "S -> QP\n",
        "QP -> SP WQ\n",
        "SP -> PRP NN\n",
        "WQ -> WP\n",
        "\n",
        "['PRP you', 'NN name',]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fP6Ks3N75uu"
      },
      "source": [
        "('S ', [('QP ', [('SP ', [('PRP you', []), ('NN name', [])]), ('WQ ', [('WP what', [])])])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ktzKsJwpNSQ"
      },
      "source": [
        "def get_terminal(tree, terminals):\n",
        "  name, children = tree\n",
        "  # print(children)\n",
        "  if not children:\n",
        "    # print(name)\n",
        "    terminals.append(name)\n",
        "    return terminals\n",
        "  for child in children:\n",
        "    terminals = get_terminal(child, terminals)\n",
        "  return terminals\n",
        "\n",
        "  "
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "C_SM9xddtZU6",
        "outputId": "e68f40b2-b9f0-4f56-9ff1-0460375fac61"
      },
      "source": [
        "from nltk.util import flatten\n",
        "t = Tree.fromstring(\"(S (QP (WQ (WP what) (FWA is)) (SP (PRPS your) (NN name))))\")\n",
        "print(flatten(t))\n",
        "' '.join(flatten(t))"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what', 'is', 'your', 'name']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what is your name'"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O6DjtYkAPYC"
      },
      "source": [
        "from nltk.util import flatten\n",
        "\n",
        "def gen_sentence(terminals, pattern):\n",
        "  tree = \"\"\n",
        "  string = \"unrecognized\"\n",
        "\n",
        "  # you name what\n",
        "  if pattern == '(S -> QP) (QP -> SP WQ) (SP -> PRP NN) (WQ -> WP)':\n",
        "    prps, nn, wp = terminals\n",
        "    prps = 'your' if prps.split(' ')[1] == 'you' else 'my' if prps.split(' ')[1] == 'i-me' else 'his-her' \n",
        "    string = f'(S (QP (WQ (WP {wp.split()[1]}) (FWA is)) (SP (PRPS {prps}) (NN {nn.split()[1]}))))'\n",
        "  # you live where\n",
        "  elif pattern == '(S -> QP) (QP -> SP VP WQ) (SP -> PRP) (VP -> VB) (WQ -> WRB)':\n",
        "    prp, vb, wrb = terminals\n",
        "    fwc = 'does' if prp.split(' ')[1] == 'he-she' else 'do' \n",
        "    string = f'(S (QP (WQ (WRB {wrb.split()[1]}) (FWC {fwc})) (SP (PRP {prp.split()[1]})) (VP (VB {vb.split()[1]}))))'\n",
        "  # you from where\n",
        "  elif pattern == '(S -> QP) (QP -> SP PP WQ) (SP -> PRP) (PP -> IN) (WQ -> WRB)':\n",
        "    prp, ins, wrb = terminals\n",
        "    prp = prp.split(' ')[1]\n",
        "    fwa = 'am' if prp == 'i-me' else 'are' if prp == 'you' else 'is'\n",
        "    string = f'(S (QP (WQ (WRB {wrb.split()[1]}) (FWA {fwa})) (SP (PRP {prp})) (PP (IN {ins.split()[1]}))))'\n",
        "  # i-me name +\n",
        "  elif pattern == '(S -> SP NNP) (SP -> PRP NN)':\n",
        "    prps, nn, nnp = terminals\n",
        "    prps = 'your' if prps.split(' ')[1] == 'you' else 'my' if prps.split(' ')[1] == 'i-me' else 'his-her' \n",
        "    string = f'(S (SP (PRPS {prps}) (NN {nn.split()[1]}) (FWA is)) (NNP {nnp.split()[1]}))'\n",
        "  # i-me good\n",
        "  elif pattern == '(S -> SP JJ) (SP -> PRP)':\n",
        "    prp, jj = terminals\n",
        "    fwa = 'am' if prp.split(' ')[1] == 'i-me' else 'are' if prp.split(' ')[1] == 'you' else 'is'\n",
        "    string = f'(S (SP (PRP {prp.split()[1]}) (FWA {fwa})) (JJ {jj.split()[1]}))'\n",
        "  # egg i-me cook\n",
        "  elif pattern == '(S -> SP VP) (SP -> NN SP) (SP -> PRP) (VP -> VB)':\n",
        "    nn, prp, vb = terminals\n",
        "    fwa = 'am' if prp.split(' ')[1] == 'i-me' else 'are' if prp.split(' ')[1] == 'you' else 'is'\n",
        "    vb = vb.split(' ')[1] \n",
        "    prp = prp.split(' ')[1]\n",
        "    vb = vb if prp == 'you' or prp == 'i-me' else vb + 's'\n",
        "    string = f'(S (SP (PRP {prp})) (VP (VB {vb}) (NN {nn.split()[1]})))'\n",
        "  # go to office\n",
        "  elif pattern == '(S -> VP PP) (VP -> VB) (PP -> IN SP) (SP -> NN)':\n",
        "    vb, ins, nn = terminals\n",
        "    string = f'(S (VP (VB {vb.split()[1]})) (PP (IN {ins.split()[1]}) (SP (NN {nn.split()[1]}))))'\n",
        "\n",
        "  tree = Tree.fromstring(string)\n",
        "  return ' '.join(flatten(tree))\n",
        "\n"
      ],
      "execution_count": 557,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrKIIn-KEYmC"
      },
      "source": [
        "where am i from\n",
        "(S (QP (WQ (WRB where) (FWA am)) (SP (PRP i-me)) (PP (IN from))))\n",
        "\n",
        "where are you from\n",
        "(S (QP (WQ (WRB where) (FWA are)) (SP (PRP you)) (PP (IN from))))\n",
        "\n",
        "where is he-she from\n",
        "(S (QP (WQ (WRB where) (FWA is)) (SP (PRP he-she)) (PP (IN from))))\n",
        "\n",
        "(S (SP (PRP i-me) (FWA am)) (JJ good))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPcvYqeudVE7",
        "outputId": "38e04f44-d467-4688-8772-8155c25d4457"
      },
      "source": [
        "text = \"go to office\"\n",
        "sent = text.split()\n",
        "# rd_parser = nltk.parse.shiftreduce.ShiftReduceParser(grammar6, trace=3)\n",
        "rd_parser = nltk.parse.recursivedescent.RecursiveDescentParser(grammar10)\n",
        "for tree in rd_parser.parse(sent):\n",
        "  print(type(tree))\n",
        "  print(tree)"
      ],
      "execution_count": 554,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.tree.Tree'>\n",
            "(S (VP (VB go)) (PP (IN to) (SP (NN office))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2zRkFp3WMQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd9275a-1517-4ef4-a355-620e3b4f9eb0"
      },
      "source": [
        "tree = None\n",
        "text = \"go to person\"\n",
        "sent = text.split()\n",
        "\n",
        "pnn_list = [word for word in sent if word not in data_list]\n",
        "pnn_list = '\\'' + '\\' | \\''.join(pnn_list) + '\\'' if len(pnn_list) else ''\n",
        "grammar = nltk.CFG.fromstring(update_grammar(word=pnn_list))\n",
        "\n",
        "# rd_parser = nltk.parse.shiftreduce.ShiftReduceParser(grammar6, trace=3)\n",
        "rd_parser = nltk.parse.recursivedescent.RecursiveDescentParser(grammar)\n",
        "for tree in rd_parser.parse(sent):\n",
        "  # print(tree)\n",
        "  tree = tree\n",
        "if tree:\n",
        "  tree = parse_root(tokenize(str(tree),' +|[A-Z a-z -]+|[()]'))\n",
        "  pattern = show_children(parse_root(tokenize(str(tree), ' +|[A-Za-z-]+|[()]')), \"\")\n",
        "  print(tree)\n",
        "\n",
        "  terminals = get_terminal(tree,[])\n",
        "\n",
        "  sentence = gen_sentence(terminals, pattern.strip())\n",
        "  print(sentence)\n",
        "else:\n",
        "  print(\"unrecognized\")\n",
        "# for node in tree:\n",
        "#   print(node)"
      ],
      "execution_count": 569,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('S ', [('VP ', [('VB go', [])]), ('PP ', [('IN to', []), ('SP ', [('NN person', [])])])])\n",
            "go to person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otifvuHhQGdd",
        "outputId": "e440c894-a414-4b39-e2a2-de2f0a0d1697"
      },
      "source": [
        "text = \"Joshua Ella Jerson\"\n",
        "sent = text.split()\n",
        "pnn_list = [word for word in sent if word not in data_list]\n",
        "pnn_list = '\\'' + '\\' | \\''.join(pnn_list) + '\\''\n",
        "grammar10 = nltk.CFG.fromstring(update_grammar(word=pnn_list))\n",
        "print(grammar)"
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 45 productions (start state = S)\n",
            "    S -> QP\n",
            "    S -> SP VP\n",
            "    S -> SP JJ\n",
            "    S -> SP PNN\n",
            "    S -> VP PP\n",
            "    S -> SP\n",
            "    QP -> SP WQ\n",
            "    QP -> SP PP WQ\n",
            "    QP -> SP VP WQ\n",
            "    SP -> PRP NN\n",
            "    SP -> PRP\n",
            "    SP -> NN\n",
            "    SP -> NN SP\n",
            "    VP -> VB RB\n",
            "    VP -> VB\n",
            "    PP -> IN SP\n",
            "    PP -> IN\n",
            "    WQ -> WP\n",
            "    WQ -> WRB\n",
            "    NN -> 'name'\n",
            "    NN -> 'egg'\n",
            "    NN -> 'office'\n",
            "    NN -> 'work'\n",
            "    NN -> 'person'\n",
            "    NN -> 'student'\n",
            "    PRP -> 'you'\n",
            "    PRP -> 'i-me'\n",
            "    PRP -> 'he-she'\n",
            "    WP -> 'what'\n",
            "    WP -> 'who'\n",
            "    WP -> 'when'\n",
            "    WRB -> 'how'\n",
            "    WRB -> 'where'\n",
            "    JJ -> 'good'\n",
            "    JJ -> 'okay'\n",
            "    IN -> 'from'\n",
            "    IN -> 'to'\n",
            "    RB -> 'here'\n",
            "    VB -> 'live'\n",
            "    VB -> 'cook'\n",
            "    VB -> 'go'\n",
            "    VB -> 'study'\n",
            "    NNP -> 'Joshua'\n",
            "    NNP -> 'Ella'\n",
            "    NNP -> 'Jerson'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFPfeF-P93YQ",
        "outputId": "71812d11-47cd-4a74-ef04-6e9e4e8ad1d5"
      },
      "source": [
        "print(f'pattern {pattern.strip()}')\n",
        "print(f'terminals {terminals}')"
      ],
      "execution_count": 552,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pattern (S -> VP PP) (VP -> VB) (PP -> IN SP) (SP -> NN)\n",
            "terminals ['VB go', 'IN to', 'NN office']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHu3z-kt6W-e"
      },
      "source": [
        "# for tree in tree:\n",
        "#   print(length)\n",
        "#   length = len(tree)\n",
        "#   while length >= 0:\n",
        "#     length = length - 1\n",
        "#     for tree in tree:      \n",
        "#       print(tree)\n",
        "\n",
        "  # for tree in tree:\n",
        "  #   for tree in tree:\n",
        "  #     for tree in tree:\n",
        "  #       print(tree)\n",
        "\n",
        "  # list = (tree)\n",
        "  # print(list)\n",
        "  # print(list[0])\n",
        "# nltk.parse.shiftreduce.demo()\n",
        "# nltk.parse.recursivedescent.demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYtFHYNFQifB",
        "outputId": "2975332c-4360-43f2-f323-64f70dbcc13a"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"Get ball\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "blob.tags           # [('The', 'DT'), ('titular', 'JJ'),\n",
        "                    #  ('threat', 'NN'), ('of', 'IN'), ...]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Get', 'VB'), ('the', 'DT'), ('ball', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJv-9JJ-YEl1",
        "outputId": "27edb517-faf7-418d-9cd8-d5e2f334018f"
      },
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}