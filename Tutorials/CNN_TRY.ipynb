{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_TRY.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIB+BIYgV3JbGgGzvuwOEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorddickenstein/FSLRwithNLP/blob/main/Tutorials/CNN_TRY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0LbBuHIEPaC"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAWiNSbeD-jb"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os  # iterate through the directories\n",
        "import cv2\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZdrVHoFE7S3"
      },
      "source": [
        "Importing the datasets from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ekcuNVZFASO"
      },
      "source": [
        "def import_data():\n",
        "    \"\"\"@doc Get the train datasets\"\"\"\n",
        "    df_train = pd.read_csv(\n",
        "        'D:/Documents/Thesis/FSLRwithNLP/Datasets/Fingerspelling/sign_mnist_train/sign_mnist_train.csv')\n",
        "    # print(df_train.head())\n",
        "\n",
        "    x_sets = df_train.drop(columns=['label'])\n",
        "    y_sets = df_train[['label']]\n",
        "\n",
        "    x_train = x_sets[:22000]\n",
        "    y_train = y_sets[:22000]\n",
        "    # print(x_train.head())\n",
        "    # print(y_train.head())\n",
        "\n",
        "    x_valid = x_sets[22001:]\n",
        "    y_valid = y_sets[22001:]\n",
        "\n",
        "    \"\"\"@doc Get the test datasets\"\"\"\n",
        "    df_test = pd.read_csv('D:/Documents/Thesis/FSLRwithNLP/Datasets/Fingerspelling/sign_mnist_test/sign_mnist_test.csv')\n",
        "    # print(df_test.head())\n",
        "\n",
        "    x_test = df_test.drop(columns=['label'])\n",
        "    y_test = df_test[['label']]\n",
        "    # print(x_test.head())\n",
        "    # print(y_test.head())\n",
        "\n",
        "    \"\"\"Convert to np array\"\"\"\n",
        "    x_train = x_train.to_numpy()\n",
        "    y_train = y_train.to_numpy()\n",
        "    x_test = x_test.to_numpy()\n",
        "    y_test = y_test.to_numpy()\n",
        "\n",
        "    \"\"\"Reshape to 28x28\"\"\"\n",
        "    num_rows_train, _ = x_train.shape\n",
        "    num_rows_test, _ = x_test.shape\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "    # print(x_train.shape)\n",
        "\n",
        "    \"\"\"Normalize data\"\"\"\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "\n",
        "    \"\"\"Convert to categorical variables\"\"\"\n",
        "    y_train = keras.utils.to_categorical(y_train, 26)\n",
        "    y_test = keras.utils.to_categorical(y_test, 26)\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et7O8N4uEZi_"
      },
      "source": [
        "Test the model from a given dataset image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbCYARD4EJk3"
      },
      "source": [
        "def test_model_from_dataset(x_train, y_train, x_test, y_test, model_name):\n",
        "    model = keras.models.load_model(model_name)\n",
        "    print(y_train[10170])\n",
        "    plt.imshow(x_train[10170], cmap='gray')\n",
        "    plt.show()\n",
        "    x_train = x_train[10170].reshape(-1, 28, 28, 1)\n",
        "    print(x_train)\n",
        "    print(x_train.shape)\n",
        "    print(x_train.ndim)\n",
        "    prediction = model.predict(x_train)\n",
        "    print(prediction)\n",
        "    class_x = np.argmax(prediction, axis=1)\n",
        "    print(find_match(class_x[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3brZxqPEfNf"
      },
      "source": [
        "Test model from an external image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VcZRakdEjpb"
      },
      "source": [
        "def test_model(img):\n",
        "    model = keras.models.load_model('Models\\FingerSpelling(32, 64, 128)_(0.4652-0.9072).h5')\n",
        "    prediction = model.predict(img)\n",
        "    print(prediction)\n",
        "    class_x = np.argmax(prediction, axis=1)\n",
        "    print(find_match(class_x[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emzc9Vm5EjN0"
      },
      "source": [
        "Find a match from the categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQpFYfemEtxV"
      },
      "source": [
        "def find_match(x):\n",
        "    spell = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E',\n",
        "             5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: ' ',\n",
        "             10: 'K', 11: 'L',12: 'M', 13: 'N', 14: 'O',\n",
        "             15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T',\n",
        "             20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y',\n",
        "             25: ' ', }\n",
        "    return spell[x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXFuMsREuUS"
      },
      "source": [
        "Show the current image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvkBwxGBEx2S"
      },
      "source": [
        "def show_image(name, img):\n",
        "    # plt.imshow(img, cmap='gray')\n",
        "    # plt.show()\n",
        "    cv2.imshow(name, img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC18XsD5E2nF"
      },
      "source": [
        "Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Y-PsQME5C5"
      },
      "source": [
        "def preprocess_image(img):\n",
        "    \"\"\"Smoothen img using Gausian blur\"\"\"\n",
        "    blur_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    # blur_img = cv2.blur(img, (5, 5), 0)\n",
        "    # blur_img = cv2.medianBlur(img, 5)\n",
        "    # show_image('blur', blur_img)\n",
        "\n",
        "    \"\"\"Threshold Image using Otsu's Binarization\"\"\"\n",
        "    _, th = cv2.threshold(blur_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    # show_image('threshold', th)\n",
        "\n",
        "    # canny edge\n",
        "\n",
        "    \"\"\"Apply morphological transformation\"\"\"\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))\n",
        "    morph = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel)\n",
        "    show_image('morph', morph)\n",
        "\n",
        "    \"\"\"Apply mask to extract object\"\"\"\n",
        "    mask = cv2.bitwise_and(img, img, mask=morph)\n",
        "    show_image('mask', mask)\n",
        "\n",
        "    \"\"\"Normalize img\"\"\"\n",
        "    norm_img = mask.astype('float32')\n",
        "    norm_img /= 255\n",
        "    show_image('normalized', norm_img)\n",
        "\n",
        "    \"\"\"Reshape img to 28x28\"\"\"\n",
        "    img_size = 28\n",
        "    resize_img = cv2.resize(norm_img, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
        "    plt.imshow(resize_img, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    \"\"\"Expand img into 4d\"\"\"\n",
        "    resize_img = np.expand_dims(resize_img, axis=(0, -1))\n",
        "    # print(img)\n",
        "    print(\"Shape\", resize_img.shape)\n",
        "    print(resize_img.ndim)\n",
        "    return resize_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DgW81I_E6C5"
      },
      "source": [
        "Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjWknG1yFJ9e"
      },
      "source": [
        "def create_model(x_train, y_train, x_test, y_test):\n",
        "    \"\"\"Create the model\"\"\"\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    # try to add with padding\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.50))\n",
        "    model.add(keras.layers.Dense(26, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam,\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_test, y_test))\n",
        "    print(model.evaluate(x_test, y_test))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5LJhcmYFKto"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Av7Ag05FMzm"
      },
      "source": [
        "def save_model(model, name):\n",
        "    model.save(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LujSQdKYFO_g"
      },
      "source": [
        "Main Configuration Area #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeQnVlEAFWjA"
      },
      "source": [
        "path = \"D:\\Documents\\Thesis\\FSLRwithNLP\\Datasets\\Test_Images\"\n",
        "file_name = \"Y2.jpg\"\n",
        "img = cv2.imread(os.path.join(path, file_name), 0)\n",
        "img = preprocess_image(img)\n",
        "test_model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfz_4gq9FfBd"
      },
      "source": [
        "Main Configuration Area #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlZhrsPjFkfb"
      },
      "source": [
        "# model_name = \"\\Models\\Fingerspelling(16, 32, 64)_(0.5030-0.9015).h5\"\n",
        "# model_name = \"D:\\Documents\\Thesis\\FSLRwithNLP\\Tutorials\\Models\\\\test.h5\"\n",
        "model_name = \"D:\\Documents\\Thesis\\FSLRwithNLP\\Tutorials\\Models\\\\test_(0.5979_0.9139).h5\"\n",
        "x_train, y_train, x_test, y_test = import_data()\n",
        "model = create_model(x_train, y_train, x_test, y_test)\n",
        "save_model(model, model_name)\n",
        "test_model_from_dataset(x_train, y_train, x_test, y_test, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}