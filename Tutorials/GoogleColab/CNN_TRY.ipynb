{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_TRY.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/Lorddickenstein/FSLRwithNLP/blob/main/Tutorials/CNN_TRY.ipynb",
      "authorship_tag": "ABX9TyN3o8PlmBmB9kvL6vqQKVSG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorddickenstein/FSLRwithNLP/blob/main/Tutorials/CNN_TRY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0LbBuHIEPaC"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAWiNSbeD-jb"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os  # iterate through the directories\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZdrVHoFE7S3"
      },
      "source": [
        "Importing the datasets from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ekcuNVZFASO"
      },
      "source": [
        "def import_data():\n",
        "    \"\"\"@doc Get the train datasets\"\"\"\n",
        "    df_train = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/Datasets/Fingerspelling/sign_mnist_train/sign_mnist_train.csv')\n",
        "    # print(df_train.head())\n",
        "\n",
        "    x_sets = df_train.drop(columns=['label'])\n",
        "    y_sets = df_train[['label']]\n",
        "\n",
        "    x_train = x_sets[:22000]\n",
        "    y_train = y_sets[:22000]\n",
        "    # print(x_train.head())\n",
        "    # print(y_train.head())\n",
        "\n",
        "    x_valid = x_sets[22001:]\n",
        "    y_valid = y_sets[22001:]\n",
        "\n",
        "    \"\"\"@doc Get the test datasets\"\"\"\n",
        "    df_test = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/Datasets/Fingerspelling/sign_mnist_test/sign_mnist_test.csv')\n",
        "    # print(df_test.head())\n",
        "\n",
        "    x_test = df_test.drop(columns=['label'])\n",
        "    y_test = df_test[['label']]\n",
        "    # print(x_test.head())\n",
        "    # print(y_test.head())\n",
        "\n",
        "    \"\"\"Convert to np array\"\"\"\n",
        "    x_train = x_train.to_numpy()\n",
        "    y_train = y_train.to_numpy()\n",
        "    x_test = x_test.to_numpy()\n",
        "    y_test = y_test.to_numpy()\n",
        "\n",
        "    \"\"\"Reshape to 28x28\"\"\"\n",
        "    num_rows_train, _ = x_train.shape\n",
        "    num_rows_test, _ = x_test.shape\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "    # print(x_train.shape)\n",
        "\n",
        "    \"\"\"Normalize data\"\"\"\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "\n",
        "    \"\"\"Convert to categorical variables\"\"\"\n",
        "    y_train = keras.utils.to_categorical(y_train, 26)\n",
        "    y_test = keras.utils.to_categorical(y_test, 26)\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et7O8N4uEZi_"
      },
      "source": [
        "Test the model from a given dataset image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbCYARD4EJk3"
      },
      "source": [
        "def test_model_from_dataset(x_train, y_train, x_test, y_test, model_name):\n",
        "    model = keras.models.load_model(model_name)\n",
        "    print(y_train[10170])\n",
        "    # plt.imshow(x_train[10170], cmap='gray')\n",
        "    # plt.show()\n",
        "    x_train = x_train[10170].reshape(-1, 28, 28, 1)\n",
        "    print(x_train)\n",
        "    print(x_train.shape)\n",
        "    print(x_train.ndim)\n",
        "    prediction = model.predict(x_train)\n",
        "    print(prediction)\n",
        "    class_x = np.argmax(prediction, axis=1)\n",
        "    print(find_match(class_x[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3brZxqPEfNf"
      },
      "source": [
        "Test model from an external image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VcZRakdEjpb"
      },
      "source": [
        "def test_model(img):\n",
        "    model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/CNN Models/FingerSpelling(32, 64, 128)_(0.4652-0.9072).h5')\n",
        "    prediction = model.predict(img)\n",
        "    print(prediction)\n",
        "    class_x = np.argmax(prediction, axis=1)\n",
        "    print(find_match(class_x[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emzc9Vm5EjN0"
      },
      "source": [
        "Find a match from the categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQpFYfemEtxV"
      },
      "source": [
        "def find_match(x):\n",
        "    spell = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E',\n",
        "             5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: ' ',\n",
        "             10: 'K', 11: 'L',12: 'M', 13: 'N', 14: 'O',\n",
        "             15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T',\n",
        "             20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y',\n",
        "             25: ' ', }\n",
        "    return spell[x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXFuMsREuUS"
      },
      "source": [
        "Show the current image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvkBwxGBEx2S"
      },
      "source": [
        "def show_image(name, img):\n",
        "    # plt.imshow(img, cmap='gray')\n",
        "    # plt.show()\n",
        "    cv2_imshow(img)\n",
        "    # cv2.waitKey(0)\n",
        "    # cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC18XsD5E2nF"
      },
      "source": [
        "Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Y-PsQME5C5"
      },
      "source": [
        "def preprocess_image(img):\n",
        "    \"\"\"Smoothen img using Gausian blur\"\"\"\n",
        "    blur_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    # blur_img = cv2.blur(img, (5, 5), 0)\n",
        "    # blur_img = cv2.medianBlur(img, 5)\n",
        "    # show_image('blur', blur_img)\n",
        "\n",
        "    \"\"\"Threshold Image using Otsu's Binarization\"\"\"\n",
        "    _, th = cv2.threshold(blur_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    # show_image('threshold', th)\n",
        "\n",
        "    \"\"\"Normalize img\"\"\"\n",
        "    norm_img = mask.astype('float32')\n",
        "    norm_img /= 255\n",
        "    # show_image('normalized', norm_img)\n",
        "\n",
        "    # canny edge\n",
        "\n",
        "    \"\"\"Apply morphological transformation\"\"\"\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))\n",
        "    morph = cv2.morphologyEx(th, cv2.MORPH_CLOSE, kernel)\n",
        "    show_image('morph', morph)\n",
        "\n",
        "    \"\"\"Apply mask to extract object\"\"\"\n",
        "    mask = cv2.bitwise_and(img, img, mask=morph)\n",
        "    show_image('mask', mask)\n",
        "\n",
        "    \"\"\"Reshape img to 28x28\"\"\"\n",
        "    img_size = 28\n",
        "    resize_img = cv2.resize(norm_img, (img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
        "    # plt.imshow(resize_img, cmap='gray')\n",
        "    # plt.show()\n",
        "\n",
        "    \"\"\"Expand img into 4d\"\"\"\n",
        "    resize_img = np.expand_dims(resize_img, axis=(0, -1))\n",
        "    # print(img)\n",
        "    print(\"Shape\", resize_img.shape)\n",
        "    print(resize_img.ndim)\n",
        "    return resize_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DgW81I_E6C5"
      },
      "source": [
        "Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjWknG1yFJ9e"
      },
      "source": [
        "def create_model(x_train, y_train, x_test, y_test):\n",
        "    \"\"\"Create the model\"\"\"\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(keras.layers.Dropout(0.25))\n",
        "    model.add(keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(keras.layers.Dropout(0.50))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.70))\n",
        "    model.add(keras.layers.Dense(26, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=64, epochs=30)\n",
        "    print(model.evaluate(x_test, y_test))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5LJhcmYFKto"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Av7Ag05FMzm"
      },
      "source": [
        "def save_model(model, name):\n",
        "    model.save(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LujSQdKYFO_g"
      },
      "source": [
        "Main Configuration Area #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeQnVlEAFWjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "17b89d43-9aea-474a-818d-6b8b57b0d378"
      },
      "source": [
        "# path = \"D:\\Documents\\Thesis\\FSLRwithNLP\\Datasets\\Test_Images\"\n",
        "file_name = 'Y2.jpg'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/Datasets/Test_Images/' + file_name\n",
        "img = cv2.imread(path, 0)\n",
        "img = preprocess_image(img)\n",
        "test_model(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAACvCAAAAAA+z/9PAAAEEUlEQVR4nO2d25LjMAhEydb+/y97X1I1s4lscWkQUPTTXBwbHTUI2ckM0Wg0Go1Go9FoNBqNWHoxjrkEx5bUfmDXz5ddKfyRHHztDykpEYSuFGQQmlIQQuhJQQqhJQUxhI6SQ2hoBYUT+lGYdCAdhHZW2EPo2iv/kiodullhagKxICzyoZkVxgnEg9DeCuMEYkLobgW1EzpR4EFYNUyNKBhqQh8KTAi9e2euE1onhGmJ7EKBDWGZEE0oCJJ9OWJDsfj/fCerzrmO8Xr8NlSSCYBaYXGyY2YwOwE4g9cpN0ggrGdKF/n6VYcoAGoCMvIzZhBBiEjaExRkTkAmxJ0OUEh4UyWeghBChBXiKSR0QjwFKQSQFTYvCKYgdkJIQgQvlfJ0QKyTjCFGUkDVBFHMrIMDKSggmBOCeWgcBY0TbAnBT/cwCroBaTfV0mEFba6Vl9FQ0ExsDAVlYZTfcTx2s4AhaMd4P04tghhyar/dhAd/dhuREH/B57uI6JXY+UvpQUeNNMAKKXeR0dJDaPSM1uCEIAoBaTfpQDYIbRJinEDmHSEoimdhY1ycDd0sJdNilq5vDLZ0SF4VbnYsXz811oTMFPibNvMoIsqCIsjnsD5OaF4dUnpBuHNvuURuEXwcAJjIgIQQRcmIB50O6aSYEwCEVFWBVQ0+I67hBOijihe4WXqfFXAOjDjloHnbLK+Ib9VIB5bN98esXEDUyAkMBIrfQAOw6zlQC4Iy6bB7xrd9+SPDMhCeBrplcFcM3qoD4X6oewab39cvjGYEpZywHi6AAajbO/ZcEoGglhO+PzKEYVDMCfQ7YHWXrD1so+A3JLyYF+QODrQDTPm2DPbYatUEkfjz2xeCwOOoGyLZ8kE0rqZOkM1t/bZ5Iam9UU7Ic59REUvDdJDPRz8ICk/ibJxjfVCNp5kTdHPaC4LS160gaHO7EwR1fWsEQV/j+0AwrHNtIFjW+i4QTP1Ojw2UseVr4QRr24uDcG4fab5yAyfY6deHAHBgeQiILKwOAVKJikPAVGMghAPLA+iSpZ2Awl4ZAsx6hSHg0q8uBGAJKgsBWYaL7iKxK1FNJ4BX45IQ0B0JEkJUtwS/TkEn4FkXhIBXPQgOSQeFEFEUPK5RzQkunItB8PFaLQhO+VapbXYrOVgnuFZGv5PXSQdHwGUgeJqsCgTXRAND8IrVtw2r4QTnVhQNwSVc73a8ghPctyQFIPhvy+AQMr3nn6v8Tpi/1hvjrOwQQrILDwEaduq/6h+koCqbGkLUSpMZQthq63EhzKehAhuOrLfXQnsuj3QADCC278xZE4J7bxcI1jFE7z9yOiFYPhCKbSXHCeQGoZYVxgnkB6GUFcYJNBCIaCAQ0UAgoqQQsv9z7ZYaCDQQiIjoH8vDgVwsHg70AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=260x175 at 0x7F0CD951ED50>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=260x175 at 0x7F0CD951EC50>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape (1, 28, 28, 1)\n",
            "4\n",
            "[[8.1856476e-29 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.5201311e-34\n",
            "  6.5799388e-10 0.0000000e+00 2.9670895e-36 9.9999917e-01 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 8.5388151e-37 4.4578540e-35 1.0332715e-21\n",
            "  7.1028357e-30 0.0000000e+00 0.0000000e+00 4.4711232e-34 7.3751440e-21\n",
            "  0.0000000e+00 0.0000000e+00 5.3041818e-35 3.0843131e-18 8.7761725e-07\n",
            "  0.0000000e+00]]\n",
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfz_4gq9FfBd"
      },
      "source": [
        "Main Configuration Area #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlZhrsPjFkfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "055896c1-2e96-484e-81ca-6e36be894fc3"
      },
      "source": [
        "# model_name = 'Fingerspelling(16, 32, 64)_(0.5030-0.9015).h5'\n",
        "model_name = 'test1.h5'\n",
        "# model_name = 'test_(0.5979_0.9139).h5'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/CNN Models/' + model_name\n",
        "x_train, y_train, x_test, y_test = import_data()\n",
        "model = create_model(x_train, y_train, x_test, y_test)\n",
        "save_model(model, path)\n",
        "test_model_from_dataset(x_train, y_train, x_test, y_test, model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        4640      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 26, 26, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               1384576   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 26)                3354      \n",
            "=================================================================\n",
            "Total params: 1,411,226\n",
            "Trainable params: 1,411,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "310/310 [==============================] - 62s 198ms/step - loss: 1.3608 - accuracy: 0.5836 - val_loss: 0.1373 - val_accuracy: 0.9705\n",
            "Epoch 2/30\n",
            "310/310 [==============================] - 61s 197ms/step - loss: 0.3062 - accuracy: 0.8946 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "310/310 [==============================] - 61s 197ms/step - loss: 0.1655 - accuracy: 0.9423 - val_loss: 0.0080 - val_accuracy: 0.9995\n",
            "Epoch 4/30\n",
            "310/310 [==============================] - 61s 197ms/step - loss: 0.1211 - accuracy: 0.9553 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "310/310 [==============================] - 61s 196ms/step - loss: 0.0960 - accuracy: 0.9660 - val_loss: 8.8265e-04 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "310/310 [==============================] - 61s 196ms/step - loss: 0.0886 - accuracy: 0.9695 - val_loss: 2.5455e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "310/310 [==============================] - 61s 196ms/step - loss: 0.0721 - accuracy: 0.9751 - val_loss: 2.5708e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "310/310 [==============================] - 60s 194ms/step - loss: 0.0638 - accuracy: 0.9771 - val_loss: 2.6971e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "310/310 [==============================] - 60s 195ms/step - loss: 0.0599 - accuracy: 0.9789 - val_loss: 1.4877e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "310/310 [==============================] - 60s 195ms/step - loss: 0.0493 - accuracy: 0.9827 - val_loss: 6.4598e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "310/310 [==============================] - 60s 195ms/step - loss: 0.0485 - accuracy: 0.9827 - val_loss: 3.3887e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "310/310 [==============================] - 61s 195ms/step - loss: 0.0452 - accuracy: 0.9841 - val_loss: 7.8802e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "310/310 [==============================] - 61s 195ms/step - loss: 0.0428 - accuracy: 0.9847 - val_loss: 3.7793e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "310/310 [==============================] - 60s 194ms/step - loss: 0.0360 - accuracy: 0.9868 - val_loss: 2.5432e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "310/310 [==============================] - 60s 193ms/step - loss: 0.0352 - accuracy: 0.9878 - val_loss: 1.0919e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "310/310 [==============================] - 60s 194ms/step - loss: 0.0326 - accuracy: 0.9880 - val_loss: 3.7741e-05 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "191/310 [=================>............] - ETA: 22s - loss: 0.0275 - accuracy: 0.9902"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-854f09bceffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/CNN Models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_model_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-a6932276aed0>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     15\u001b[0m     model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n\u001b[1;32m     16\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}